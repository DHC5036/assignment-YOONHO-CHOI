1. 
1)Adamax : AdaMax는 Adam의 확장으로 제안된 알고리즘으로 Adam의 경우 L2-norm을 기반으로 학습률을 조절하는 데 반해 AdaMax의 경우 Lp-norm을 이용한다. 
단 p가 매우 클 경우 Lp-norm은 극단적인 값을 갖는 등의 매우 불안정한 모습을 보여준다. Adam 논문의 저자는 p가 무한대로 갈 때 매우 간단하고 안정적인 알고리즘이 만들어진다고 언급한다.
2)Nadam : Nadam은 NAG와 Adam의 개념을 합친 옵티마이저로 현재 위치에서 다음 위치로 이동할 기울기와 모멘텀 값을 구하는 것이 아닌 모멘텀 값으로 이동한 뒤에 기울기를 구하는 방식이다. 
Adam에서는 m(첫 번째 모멘텀)과 v(두 번째 모멘텀)를 사용하는데, m은 기울기 값을 좀 더 빠르게 계산할 수 있도록 돕는 역할이고, v는 데이터의 분포가 희소한 곳에서 영향력을 극대화함으로써 
희소한 영역을 벗어날 수 있게 도움을 주는 역할을 한다. Nadam은 여기서 NAG의 철학을 이용하여, 현재 위치가 아닌 모멘텀 방향으로 이동한 뒤의 위치에서 기울기 값을 구하며, 
이 때문에 1차 편미분(야코비안, Jacobian)을 계산할 때, Adam과는 다른 위치 상에서 계산한다. 
Nadam은 Adam과 NAG의 장점을 합쳤기 때문에 Adam보다 좀 더 빠르고 정확하게 전역 최솟값(global minimum)을 찾아낼 수 있다는 장점이 있다.
